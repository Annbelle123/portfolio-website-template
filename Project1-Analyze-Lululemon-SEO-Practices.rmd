---
title: "Analyzing Lululemon SEO Practices Against Competitors"
author: "Annabelle Petherbridge"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: readable
    highlight: null
---

## Methodology and Input Companiesâ€™ Websites

### Methodology

A reproducible research document was developed to systematically scrape and analyze websites from various competitors across four main types of webpages: Home, About, Sustainability, and Customer Service. The table below details the inputs used for this analysis.

The process began by gathering the inputs, which included URLs corresponding to the four page types for each competitor. Each webpage was then scraped to extract headings and title text, resulting in a structured dataset containing these elements for Lululemon, Vuori, and Athleta.

Subsequently, text mining techniques were applied, focusing on bi-gram frequency analysis. Bi-grams (pairs of adjacent words) were generated from the headings and title text found on the webpages. This approach simulates the indexing process performed by search engines, providing insights into the terms and phrases prioritized by different organizations in their digital content.

The reproducible nature of this research document ensures that the analysis can be effortlessly rerun for any company with similar webpages. This flexibility makes it a valuable tool for benchmarking website content and SEO strategies within the context of competitive analysis.

### Input Companiesâ€™ Websites

```{r seo-analysis-all-in-one, message=FALSE, warning=FALSE}


```

# **Assessment of SEO Practices**

This section will contain the detailed analysis of the SEO practices for each brand based on the scraped data.

## **ðŸ§  Scraping Function**

```{r}
# Load libraries
library(RSelenium)
library(rvest)
library(dplyr)
library(tibble)
library(tidyr)
library(knitr)
library(tidytext)
library(stringr)

# Input URLs
seo_pages <- tibble::tibble(
  Brand = c(rep("Lululemon", 4), rep("Vuori", 4), rep("Athleta", 4)),
  Page_Type = rep(c("Home", "About", "Sustainability", "Customer Service"), 3),
  URL = c(
    "https://shop.lululemon.com",
    "https://shop.lululemon.com/about/strategic-sales/corporate",
    "https://shop.lululemon.com/about/responsibility",
    "https://shop.lululemon.com/help",
    "https://vuoriclothing.com",
    "https://vuoriclothing.com/pages/about-us",
    "https://vuoriclothing.com/pages/responsibility",
    "https://vuoriclothing.com/pages/contact-us",
    "https://athleta.gap.com",
    "https://athleta.gap.com/customerService/info.do?cid=2336",
    "https://athleta.gap.com/browse/info.do?cid=1094461",
    "https://athleta.gap.com/customerService/info.do?cid=2336"
  )
)

# Start RSelenium
rD <- rsDriver(browser = "chrome", port = 4545L, chromever = "latest", verbose = FALSE)
remDr <- rD$client

# Scraping function (no URL column returned)
scrape_seo_rselenium <- function(url) {
  tryCatch({
    remDr$navigate(url)
    Sys.sleep(5)
    page_source <- remDr$getPageSource()[[1]]
    page <- read_html(page_source)

    tibble(
      h1 = page %>% html_elements("h1") %>% html_text(trim = TRUE) %>% paste(collapse = " | "),
      h2 = page %>% html_elements("h2") %>% html_text(trim = TRUE) %>% paste(collapse = " | "),
      h3 = page %>% html_elements("h3") %>% html_text(trim = TRUE) %>% paste(collapse = " | "),
      h4 = page %>% html_elements("h4") %>% html_text(trim = TRUE) %>% paste(collapse = " | "),
      h5 = page %>% html_elements("h5") %>% html_text(trim = TRUE) %>% paste(collapse = " | "),
      h6 = page %>% html_elements("h6") %>% html_text(trim = TRUE) %>% paste(collapse = " | "),
      titleText = page %>% html_element("title") %>% html_text(trim = TRUE)
    )
  }, error = function(e) {
    message(paste("Failed to scrape:", url))
    return(tibble(h1=NA, h2=NA, h3=NA, h4=NA, h5=NA, h6=NA, titleText=NA))
  })
}

# Scrape all pages
seo_data <- seo_pages %>%
  rowwise() %>%
  mutate(data = list(scrape_seo_rselenium(URL))) %>%
  unnest(cols = c(data), names_sep = "_") %>%
  ungroup()

# Stop RSelenium safely
tryCatch({
  remDr$close()
  rD$server$stop()
}, error = function(e) message("RSelenium session already closed or failed to start."))

# Split by page type
home_data <- filter(seo_data, Page_Type == "Home")
about_data <- filter(seo_data, Page_Type == "About")
sustainability_data <- filter(seo_data, Page_Type == "Sustainability")
customer_service_data <- filter(seo_data, Page_Type == "Customer Service")

# SEO summary function
generate_seo_summary <- function(data, page_name) {
  heading_counts <- data %>%
    select(starts_with("data_h")) %>%
    summarise(across(everything(), ~ sum(!is.na(.) & . != ""))) %>%
    pivot_longer(cols = everything(), names_to = "Heading", values_to = "Count")

  all_text <- data %>%
    select(starts_with("data_h"), data_titleText) %>%
    pivot_longer(cols = everything(), values_to = "text") %>%
    filter(!is.na(text)) %>%
    pull(text) %>%
    paste(collapse = " ")

  bigrams <- tibble(text = all_text) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    count(bigram, sort = TRUE) %>%
    filter(n > 1)

  cat("###", page_name, "\n\n")
  cat("**Heading Structure:**\n")
  print(kable(heading_counts))

  cat("\n**Most Frequent Bi-grams:**\n")
  print(kable(head(bigrams, 10)))

  cat("\n**SEO Observations:**\n")
  cat("- Title text could be more specific and keyword-rich.\n")
  cat("- Consider emphasizing terms like 'donations', 'training', or 'certification' if relevant.\n")
  cat("- Ensure high-level headings (H1, H2) reflect core mission and services.\n\n")
}

# Run analysis
generate_seo_summary(home_data, "Home Page")
generate_seo_summary(about_data, "About Page")
generate_seo_summary(sustainability_data, "Sustainability Page")
generate_seo_summary(customer_service_data, "Customer Service Page")
```

### **ðŸ”„ Scrape All Pages**

```{r}

```

### **ðŸ§¾ Split by Page Type**

```{r}

```

### **ðŸ“Š SEO Summary Function**

```{r}

```
